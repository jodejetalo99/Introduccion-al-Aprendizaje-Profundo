{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "IPA_T1_E3_JJTL.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPTXAp1HbzMYqREfyRhvbvV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jodejetalo99/Introduccion-al-Aprendizaje-Profundo/blob/main/IPA_T1_E3_JJTL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "92eGzdrmt2_V"
      },
      "source": [
        "# Universidad Nacional Autónoma de México\n",
        "# Instituto de Investigaciones en Matemáticas Aplicadas y en Sistemas\n",
        "# Introducción al Aprendizaje Profundo\n",
        "# José de Jesús Tapia López\n",
        "# Tarea 1: Preceptrón y Redes Densas\n",
        "# 19 de Marzo del 2021"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NjQEr534t-WR"
      },
      "source": [
        "## Ejercicio 3\n",
        "\n",
        "3. Entrena una red completamente conectada para aproximar la compuerta XOR."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x2TRlos7twRf"
      },
      "source": [
        "import random\n",
        "# redes neuronales\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torchvision.transforms as T\n",
        "# algebra lineal\n",
        "import numpy as np"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WtL6Foa2uEuS"
      },
      "source": [
        "def set_seed(seed=171299):\n",
        "    \"\"\"Initializes pseudo-random number generators.\"\"\"\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "\n",
        "    \n",
        "# reproducibilidad\n",
        "set_seed()"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5k7UdZ_x05cy"
      },
      "source": [
        "La siguiente función es para la salida de la XOR. Como podremos observar más adelante la compuerta XOR, al final se le aplica una función sigmoide, con el objetivo de separar las etiquetas de 0 y 1. Si al aplicar por último la función sigmoide, el valor es mayor a 0.5, la etiqueta es 1, y si es menor o igual a 0.5, la etiqueta es cero.(Se está viendo como probabilidades)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MKFAZPI_uHd3"
      },
      "source": [
        "def step(z):\n",
        "    \"\"\"Computes step function.\"\"\"\n",
        "    return 1.0 if z > 0.5 else 0.0"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ajHKA1_7uJVN"
      },
      "source": [
        "# vectorizamos la función para que\n",
        "# aplique a un arreglo de entradas\n",
        "step_vec = np.vectorize(step)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7HUGL1DzuNm0",
        "outputId": "3bd5b416-94c1-45dd-af0f-50c05136993f"
      },
      "source": [
        "class XOR(nn.Module):\n",
        "    \n",
        "    def __init__(self):   \n",
        "        \"\"\"\n",
        "        Inicialización de las capas del modelo\n",
        "        \"\"\"\n",
        "        super(XOR, self).__init__()\n",
        "        self.fc1 = nn.Linear(2,2) \n",
        "        self.fc2 = nn.Linear(2,1)\n",
        "     \n",
        "    def forward(self,x):\n",
        "        \"\"\"\n",
        "        Definición de la estructura del modelo \n",
        "        (se aplica como función de activación una sigmoide)\n",
        "        Parámetros: \n",
        "        x (tensor): entrada del modelo\n",
        "        Devuelve: \n",
        "        y_pred: predicción\n",
        "        \"\"\"\n",
        "        x_in = torch.sigmoid(self.fc1(x)) \n",
        "        y_pred = torch.sigmoid(self.fc2(x_in)) \n",
        "        return y_pred\n",
        "    \n",
        "xor = XOR()\n",
        "\n",
        "# Datos de entrenamiento\n",
        "x = torch.tensor([[0.,0.],[0.,1.],[1.,0.],[1.,1.]])\n",
        "y = torch.tensor([[0.],[1.],[1.],[0.]])\n",
        "\n",
        "# Usamos el error cuadrático medio como criterio\n",
        "criterio = nn.MSELoss()                            \n",
        "# optimizador Adam (método de optimización estocástica) modifica la tasa de aprendizaje\n",
        "optimizador = optim.Adam(xor.parameters(), lr=0.1)  \n",
        "\n",
        "epocas = 1000\n",
        "\n",
        "for e in range(epocas):\n",
        "    # Establece los gradientes de todos los tensores optimizados a cero.\n",
        "    optimizador.zero_grad()\n",
        "    # salida de la XOR    \n",
        "    salida = xor(x)\n",
        "\n",
        "    # forward, backward y optimización\n",
        "\n",
        "    # función de perdida basada en el criterio\n",
        "    perdida = criterio(salida, y)\n",
        "    if e % 100 == 0:\n",
        "      print(\"Época:{0}, Pérdida: {1}\".format(e,perdida))\n",
        "    # Los gradientes son \"almacenados\" por los propios tensores (\n",
        "    # una vez que llama backward() en la pérdida \n",
        "    perdida.backward()\n",
        "    # hace que el optimizador itere sobre todos los parámetros (tensores) \n",
        "    # que se supone debe actualizar\n",
        "    optimizador.step() \n",
        "\n",
        "print(\"\\nCriterio: MSE,  Optimizador: Adam\", \"lr = 0.1\")\n",
        "prueba = xor(x)\n",
        "print('\\nx_1 \\tx_2 \\ty\\ty_hat')\n",
        "print('-----------------------------')\n",
        "for i in range(x.shape[0]):\n",
        "    x1, x2 = x[i]\n",
        "    y_hat = step_vec(prueba.detach())\n",
        "    print(f'{x1}\\t{x2}\\t{y[i][0]}\\t{y_hat[i][0]}')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Época:0, Pérdida: 0.2615337371826172\n",
            "Época:100, Pérdida: 0.005222637206315994\n",
            "Época:200, Pérdida: 0.0011664106277748942\n",
            "Época:300, Pérdida: 0.0006026178598403931\n",
            "Época:400, Pérdida: 0.00037941010668873787\n",
            "Época:500, Pérdida: 0.0002648420922923833\n",
            "Época:600, Pérdida: 0.0001970729645108804\n",
            "Época:700, Pérdida: 0.00015317954239435494\n",
            "Época:800, Pérdida: 0.00012288903235457838\n",
            "Época:900, Pérdida: 0.00010098466009367257\n",
            "\n",
            "Criterio: MSE,  Optimizador: Adam lr = 0.1\n",
            "\n",
            "x_1 \tx_2 \ty\ty_hat\n",
            "-----------------------------\n",
            "0.0\t0.0\t0.0\t0.0\n",
            "0.0\t1.0\t1.0\t1.0\n",
            "1.0\t0.0\t1.0\t1.0\n",
            "1.0\t1.0\t0.0\t0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J4j-yCCGuOCi"
      },
      "source": [
        "Podemos observar que esta red aproxima bien la compuerta XOR prácticamente a partir de la época 100 con una pérdida que ya empieza a acercarse mucho a 0. Sin embargo, se intentó usar como optimizador el SGD pero requería de muchas más épocas (más de 10000) para aproximar bien a dicha compuerta (la pérdida disminuía más lentamente):\n",
        "\n",
        "![](https://raw.githubusercontent.com/jodejetalo99/Introduccion-al-Aprendizaje-Profundo/main/Figuras/T1/IPA_T1_E3_1.PNG)\n",
        "\n",
        "\n",
        "Más aún, con una tasa de aprendizaje (lr) de 0.01, tanto en el optimizador Adam como en el SGD se tardaban más en disminuir la pérdida y la red no aproximaba bien la compuerta XOR:\n",
        "\n",
        "![](https://raw.githubusercontent.com/jodejetalo99/Introduccion-al-Aprendizaje-Profundo/main/Figuras/T1/IPA_T1_E3_4.PNG)\n",
        "![](https://raw.githubusercontent.com/jodejetalo99/Introduccion-al-Aprendizaje-Profundo/main/Figuras/T1/IPA_T1_E3_3.PNG)\n",
        "\n",
        "Aún así, en las imágenes anteriores observamos que el optimizador ADAM es más eficiente, pues la pérdida la disminuye más rápido que con SGD.\n",
        "\n",
        "\n",
        "Creo que esto se debe a que las tasas de aprendizaje más pequeñas requieren más épocas de entrenamiento (requiere más tiempo para entrenar) debido a los cambios más pequeños realizados en los pesos en cada actualización, mientras que las tasas de aprendizaje más grandes dan como resultado cambios rápidos y requieren menos épocas de entrenamiento. Sin embargo, las tasas de aprendizaje más altas a menudo dan como resultado un conjunto final de pesos subóptimo. Por ello, creo que en principio un lr de 0.1 fue un número adecuado."
      ]
    }
  ]
}